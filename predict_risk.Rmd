---
title: "Development, internal-external validation, and use of a prognostic model to predict future foot complications among people with diabetes recently discharged from hospital in Ontario, Canada"
always_allow_html: true
output:
  github_document:
    toc: true
    toc_depth: 3
    keep_md: true
    pandoc_args: ["--webtex"]
---

# Overview

This file contains R code illustrating how to:

1. Obtain all relevant model information (coefficients, hyperparameters etc.) from the model object saved in this repository.
2. Use the model object to generate predictions on new test data.
3. Assess model performance on the new test data.

# Set-up

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 5, fig.height = 5,
  fig.path = "figs/"
)
```

To run the code below, please install and load the following R libraries. To use the same package versions that were used for model development, please see [Session Info](#session-info) at the end of this document:

```{r}
library(data.table)
library(dplyr)
library(geepack)
library(ggplot2)
library(prodlim)
library(riskRegression)
library(rms)
library(tableone)
```


# Fine-Gray Regression (FGR) Model

The final FGR model (trained on the whole cohort reported in Roberts & Loeffler et al.) is saved as an .rds file in this repository and can be loaded as follows: 

```{r}
model <- readRDS("model/final_FGR_clean.rds")
```

The model was optimized using `riskRgression::FGR()` (wrapper for `cmprsk::crr()` to ensure compatibility with `riskRgression::Score()` - see section below on obtaining model performance metrics). The FGR model object contains all relevant items returned by `riskRegression::FGR()`, including model coefficients (and bootstrapped SEs) as well as all knot locations for continuous predictors, which were modeled with restricted cubic splines using `rms::rcs()`:

```{r}
names(model)
# crrFit = model fit obtained with riskRegression::FGR()
# call = original model call (note: variance was set to FALSE, SEs of coefficients were obtained using bootstrapping)
# terms = model terms
# form = model formula
# cause = 1 (event of interest: 1 = foot complication)
# coef = optimized model coefficients (SEs) and corresponding subdistribution hazard ratios (sHR)
# splines = knot locations for restricted cubic splines for continuous predictors
```

# Generating predictions on new data

The model object can be used to generate predicted risk scores on new data (and subsequently evaluate model performance - see section below).

**For more information on relevant predictor variables and how they are defined, please carefully review the data dictionary in `data/data_dictionary.csv`.**

<details><summary>Expand this section to review the data dictionary</summary>

#### Outcomes

```{r echo = FALSE}
data_dictionary <- read.csv("data/data_dictionary.csv") %>%
  data.table()

knitr::kable(data_dictionary[modelvar == "outcome", -("modelvar")])
```

#### Predictors

```{r echo = FALSE}
knitr::kable(data_dictionary[modelvar == "predictor", -("modelvar")])
```
</details>
<br>

Let's create some new data from a single (hypothetical) patient:

```{r}
# create data for a single patient
new_data <- data.frame(
  age = 67,
  sex_f = 1,
  elective_adm = 1,
  homelessness = 0,
  peripheral_AD = 0,
  coronary_AD = 1,
  stroke = 0,
  CHF = 0,
  hypertension = 1,
  COPD = 0,
  CKD = 0,
  malignancy = 0,
  mental_illness = 0,
  creatinine = 140.0,
  Hb_A1C = 8.5,
  albumin = 32.1,
  Hb_A1C_missing = 0,
  creatinine_missing = 0,
  albumin_missing = 0
)
```

Crucially, in our FGR model, continuous variables (age, hemoglobin A1C, creatinine, and albumin) were modeled using restricted cubic splines. We therefore need to derive the spline basis functions based on the knot locations stored in the `FGR` model object:

```{r}
# derive splines based on knot locations
age_splines <- rcs(new_data$age, model$splines$age_knots)
creatinine_splines <- rcs(new_data$creatinine, model$splines$creatinine_knots)
Hb_A1C_splines <- rcs(new_data$Hb_A1C, model$splines$hba1c_knots)
albumin_splines <- rcs(new_data$albumin, model$splines$albumin_knots)

# add spline terms to new_data
new_data$age1 <- age_splines[, 2]
new_data$age2 <- age_splines[, 3]
new_data$creatinine1 <- creatinine_splines[, 2]
new_data$creatinine2 <- creatinine_splines[, 3]
new_data$Hb_A1C1 <- Hb_A1C_splines[, 2]
new_data$Hb_A1C2 <- Hb_A1C_splines[, 3]
new_data$albumin1 <- albumin_splines[, 2]
```

We can now predict the risk of this patient for developing foot complications (cause = 1) within 1 year:

```{r, results='hide'}
# predict risk of foot complication at 1 year
risk_1_year <- predict(model, newdata = new_data, times = 365.25)[1]
```

```{r}
print(risk_1_year)
```

Thus, our hypothetical patient has a risk of 1.72% of developing a diabetic foot complication within 1 year.

We can also plot the cumulative incidence function (CIF) for this patient as follows:

```{r, results='hide'}
# to plot CIF, we need to extract predicted values at multiple time points
time <- seq(1, 365 * 5, 5) # predict up to 5 years in steps of 5 days
p <- predict(model, newdata = new_data, times = time)
```

```{r}
plot(time, p, type = "l")
```


# Evaluating model performance

If you want to evaluate model performance on a new dataset, we recommend following the methods reported in [van Geloven et al., 2022](https://www.bmj.com/content/377/bmj-2021-069249). Detailed code examples can be found in the [survival-lumc repository](https://github.com/survival-lumc/ValidationCompRisks). Here, we just provide a brief summary of how to obtain AUROC and the calibration plot/metrics reported in Roberts & Loeffler et al. (in prep.)

We'll use some (randomly generated) dummy data here to illustrate how you can obtain AUROC and calibration metrics reported in Roberts & Loeffler et al. (in preparation):

```{r}
# dummy data generated with /data/simulate_data.R
dummy_data <- readRDS("data/dummy_data.rds") 
```


## Data pre-processing

The characteristics of our dummy cohort are similar to the cohort reported by Roberts & Loeffler et al.:

<details><summary>Show code</summary>


```{r, collape=TRUE}
tab1 <- CreateTableOne(vars = c(
  "age",
  "sex_f",
  "elective_adm",
  "peripheral_AD",
  "coronary_AD",
  "stroke",
  "CHF",
  "hypertension",
  "COPD",
  "CKD",
  "malignancy",
  "mental_illness",
  "homelessness",
  "Hb_A1C",
  "creatinine",
  "albumin"
), data = dummy_data)

tab1 <- print(
  tab1,
  nonnormal = c("age", "Hb_A1C", "creatinine", "albumin"),
  showAllLevels = FALSE,
  printToggle = FALSE
)
```
</details>

```{r echo=FALSE}
# remove extra space after parentheses
tab1 <- apply(tab1, 2, function(col) {
  gsub("\\(\\s*(\\d)", "(\\1", col)
})

knitr::kable(tab1)
```


Note that the lab results (Hb A1c, creatinine, and albumin) have missing values. To make sure we obtain valid model performance metrics, we use the same imputation method that was used to develop and validate the original model. Specifically, we create a new variable indicating missingness of lab values, and then set the corresponding test results to 0:

```{r}
# create missing indicator flag
dummy_data[, Hb_A1C_missing := ifelse(is.na(Hb_A1C), TRUE, FALSE)]
dummy_data[, creatinine_missing := ifelse(is.na(creatinine), TRUE, FALSE)]
dummy_data[, albumin_missing := ifelse(is.na(albumin), TRUE, FALSE)]

# set missing test results to 0
dummy_data[is.na(Hb_A1C), Hb_A1C := 0]
dummy_data[is.na(creatinine), creatinine := 0]
dummy_data[is.na(albumin), albumin := 0]
```


Additionally, we again need to add the spline terms for each continuous variable. Here, we are applying the same knot locations that were used in the validated model:

```{r}
age_splines <- rcs(dummy_data$age, model$splines$age_knots)
creatinine_splines <- rcs(dummy_data$creatinine, model$splines$creatinine_knots)
Hb_A1C_splines <- rcs(dummy_data$Hb_A1C, model$splines$hba1c_knots)
albumin_splines <- rcs(dummy_data$albumin, model$splines$albumin_knots)

dummy_data[, age1 := age_splines[, 2]]
dummy_data[, age2 := age_splines[, 3]]
dummy_data[, creatinine1 := creatinine_splines[, 2]]
dummy_data[, creatinine2 := creatinine_splines[, 3]]
dummy_data[, Hb_A1C1 := Hb_A1C_splines[, 2]]
dummy_data[, Hb_A1C2 := Hb_A1C_splines[, 3]]
dummy_data[, albumin1 := albumin_splines[, 2]]
```


## Performance metrics

To obtain model performance metrics, we can use the `riskRegression::Score()` function:

```{r, results='hide'}
# get performance at 1 year
t <- 365.25

score_result <- Score(
  list(cr_model = model),
  data = dummy_data,
  formula = Hist(time, status) ~ 1,
  cause = 1,
  times = t,
  metrics = c("auc"),
  summary = c("risks", "ipa"),
  plots = c("ROC", "calibration"),
  conf.int = TRUE
)
```

### AUROC

To plot AUROC at 1 year, we can run `plotROC()` on the output returned by `Score()`:

```{r}
plotROC(score_result,
  times = t,
  ylab = paste0("Sensitivity at 1 year"),
  xlab = paste0("1-Specificity at 1 year")
)
```

### Calibration

Similarly, we can create a calibration plot based on the output of `Score()` by running `plotCalibration()`:

```{r}
# calibration plot
plotCalibration(
  score_result,
  method = "nne", # default: nearest neighborhood smoothing
  cens.method = "jackknife",
  round = FALSE,
  xlim = c(0, 0.05),
  ylim = c(0, 0.05),
  rug = TRUE
)
```


Note that `plotCalibration()` does not provide an option for loess smoothing of the calibration curves. However, we can easily create loess-smoothed curves with additional customization based on the data returned by `Score()`:

```{r}
# get predicted/observed values at 1 year
obs <- score_result$Calibration$plotframe[times == t]$pseudovalue
pred <- score_result$Calibration$plotframe[times == t]$risk

# for plotting purposes, only include predicted scores within 99.9th percentile
lim <- quantile(pred, 0.999)

# get density of predicted values
d <- density(pred)
d_scaled <- lim - (d$y / max(d$y) * (lim / 4))
density_data <- data.frame(x = d$x, y = d_scaled)

# use loess smoothing
smooth_pseudos <- predict(
  stats::loess(obs ~ pred, degree = 1, span = 2 / 3),
  se = TRUE
)

CI_lower <- smooth_pseudos$fit - qt(0.975, smooth_pseudos$df) * smooth_pseudos$se
CI_upper <- smooth_pseudos$fit + qt(0.975, smooth_pseudos$df) * smooth_pseudos$se

fig <- ggplot() +
  geom_ribbon(aes(x = pred, y = obs, ymin = CI_lower, ymax = CI_upper), alpha = 0.3, fill = "darkblue") +
  geom_line(aes(x = pred, y = smooth_pseudos$fit), color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, color = "black", linetype = 2) +
  geom_ribbon(data = density_data, aes(x = x, ymin = y, ymax = lim), alpha = 0.3) +
  geom_line(data = density_data, aes(x = x, y = y)) +
  scale_x_continuous("Estimated risk at 1 year", expand = c(0, 0)) +
  scale_y_continuous(paste0("Observed outcome proportions at 1 year"), expand = c(0, 0)) +
  coord_cartesian(xlim = c(-0.001, lim), ylim = c(-0.001, lim)) +
  theme_bw() +
  theme(aspect.ratio = 1)

print(fig)
```


#### Calibration metrics

In addition to the calibration plot, we can evaluate calibration by calculating the calibration slope & intercept, integrated calibration index (ICI), and E50/E90 (50th/90th percentile of the absolute prediction error).
The following code was adapted from the [survival-lumc repository](https://github.com/survival-lumc/ValidationCompRisks) (also see [van Geloven et al., 2022](https://www.bmj.com/content/377/bmj-2021-069249)).


**Calibration slope**

To fit a model for the calibration slope, we use a complementary log-log transformation of the predicted risk estimates and use generalized estimating equations (GEE) to obtain robust standard errors via jackknife resampling (used for pseudovalue estimation):

```{r}
# prepare data
data <- data.table(
  pred = pred,
  obs = obs,
  pred_cll = log(-log(1 - pred)) # get cloglog of predicted risk
)

# fit model for calibration slope
fit_cal_slope <- geese(
  obs ~ offset(pred_cll) + pred_cll,
  data = data,
  id = 1:nrow(data),
  scale.fix = TRUE,
  family = gaussian,
  mean.link = "cloglog", # link function for the means: complementary log-log transformation
  corstr = "independence",
  jack = TRUE # SE's are estimated using jackknife resampling method
)

# get model summary
cal_slope <- summary(fit_cal_slope)$mean

# combine all
calibration_slope <- data.table(
  slope = 1 + cal_slope["pred_cll", ]$estimate,
  slope_SE = cal_slope["pred_cll", ]$san.se,
  slope_CI_lower = 1 + cal_slope["pred_cll", ]$estimate - qnorm(0.975) * cal_slope["pred_cll", ]$san.se,
  slope_CI_upper = 1 + cal_slope["pred_cll", ]$estimate + qnorm(0.975) * cal_slope["pred_cll", ]$san.se
)

print(calibration_slope)
```


**Calibration intercept**

To get the calibration intercept, we use the same model as above, but fix the slope to 1. This will tell us to what extend the model generally over-/underestimates risk scores:

```{r}
# fit model for calibration intercept
fit_cal_int <- geese(
  obs ~ offset(pred_cll),
  data = data,
  id = 1:nrow(data),
  scale.fix = TRUE,
  family = gaussian,
  mean.link = "cloglog",
  corstr = "independence",
  jack = TRUE
)

# get model summary
cal_int <- summary(fit_cal_int)$mean

# combine all
calibration_intercept <- data.table(
  intercept = cal_int$estimate,
  intercept_se = cal_int$san.se,
  intercept_CI_lower = cal_int$estimate - qnorm(0.975) * cal_int$san.se,
  intercept_CI_upper = cal_int$estimate + qnorm(0.975) * cal_int$san.se
)

print(calibration_intercept)
```

**ICI and E50/90**

Finally, we calculate ICI and E50/E90 to obtain summary metrics of the difference between predicted vs. observed risk scores:

```{r}
# get absolute prediction error
abs_pred_error <- abs(obs - pred)

ICI <- mean(abs(abs_pred_error))
E50 <- quantile(abs(abs_pred_error), c(0.5))
E90 <- quantile(abs(abs_pred_error), c(0.9))
```

```{r echo = FALSE}
print(paste("ICI = ", round(ICI, 3)))
print(paste("E50 = ", round(E50, 3)))
print(paste("E90 = ", round(E90, 3)))
```

# Session Info

```{r}
print(sessionInfo())
```
